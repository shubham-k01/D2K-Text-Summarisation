{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1NniMSp4O46hw5ki1oze1KgXg1xi_4FF5",
      "authorship_tag": "ABX9TyN5d/tLaB9TOX7M/FX3Ciga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubham-k01/D2K-Text-Summarisation/blob/main/D2K_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting the data**"
      ],
      "metadata": {
        "id": "W8NpvPg9NACf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1eejOkiXcIS",
        "outputId": "5b9b3863-282a-4327-b750-66588bed4088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00107\n",
            "\n",
            "https://downloads.hindawi.com/journals/cin/2022/5624475.pdf\n",
            "\n",
            "https://pubs.acs.org/doi/pdf/10.1021/acssynbio.2c00015\n",
            "\n",
            "https://downloads.hindawi.com/journals/cin/2022/5624475.pdf\n",
            "\n",
            "https://link.springer.com/article/10.1007/s11023-021-09573-8\n",
            "\n",
            "https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0237749&type=printable\n",
            "\n",
            "https://www.mdpi.com/2079-4991/12/15/2646\n",
            "\n",
            "https://pubs.acs.org/doi/10.1021/acs.chemrev.0c00004\n",
            "\n",
            "https://www.hindawi.com/journals/jhe/2022/4653923/\n",
            "\n",
            "https://www.mdpi.com/1660-4601/19/22/15027\n",
            "\n",
            "https://www.nature.com/articles/s41598-021-84637-4\n",
            "\n",
            "https://onlinelibrary.wiley.com/doi/10.1111/1754-9485.13274\n",
            "\n",
            "https://www.mdpi.com/1424-8220/22/23/9148\n",
            "\n",
            "https://www.frontiersin.org/articles/10.3389/fnagi.2021.633752/full\n",
            "\n",
            "https://pubs.asahq.org/anesthesiology/article/132/2/379/108833/Artificial-Intelligence-in-AnesthesiologyCurrent\n",
            "\n",
            "https://www.mdpi.com/2072-6694/14/6/1524\n",
            "\n",
            "https://translational-medicine.biomedcentral.com/articles/10.1186/s12967-022-03666-3\n",
            "\n",
            "https://onlinelibrary.wiley.com/doi/10.1111/jan.14855\n",
            "\n",
            "https://www.frontiersin.org/articles/10.3389/fpsyg.2022.898107/full\n",
            "\n",
            "https://www.nature.com/articles/nrc.2017.69\n",
            "\n",
            "https://www.mdpi.com/1422-0067/23/12/6813\n",
            "\n",
            "https://acsjournals.onlinelibrary.wiley.com/doi/full/10.3322/caac.21405\n",
            "\n",
            "https://acsjournals.onlinelibrary.wiley.com/doi/10.1002/cncr.32887\n",
            "\n",
            "https://onlinelibrary.wiley.com/doi/10.1002/ijc.32892\n"
          ]
        }
      ],
      "source": [
        "with open('/content/PS3_Dataset.txt') as f:\n",
        "  l = f.read()\n",
        "  # l = l.replace(' ',',')\n",
        "  print(l)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Converting PDF to text**"
      ],
      "metadata": {
        "id": "5Lfgw-akNF5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "9pkhfJ0voZA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        " \n",
        "# creating a pdf reader object\n",
        "reader = PdfReader('/content/drive/MyDrive/D2K_Data/sample0.pdf')\n",
        " \n",
        "# printing number of pages in pdf file\n",
        "print(len(reader.pages))\n",
        " \n",
        "# getting a specific page from the pdf file\n",
        "page = reader.pages[0]\n",
        " \n",
        "# extracting text from page\n",
        "text = page.extract_text()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qxe2_0KcpgXl",
        "outputId": "f4a10f68-87f0-468a-90d8-55856b9e0440"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "Health Policy and Technology  10 (2021) 209–229  \n",
            "Contents  lists available  at ScienceDirect  \n",
            "Health  Policy  and Technology  \n",
            "journal  homepage:  www.elsevier.com/locate/hlpt  \n",
            "Literature  Review  \n",
            "Telemedicine  and health  policy:  A systematic  review  \n",
            "Clemens  Scott Kruse  ∗, Kelly Williams  1 , John Bohls  1 , Waleed  Shamsi  1 \n",
            "School of Health Administration,  Texas State University,  San Marcos, TX, United States \n",
            "a r t i c l e i n f o \n",
            "Article history: \n",
            "Available  online 29 October 2020 \n",
            "Keywords:  \n",
            "Health \n",
            "Policy \n",
            "Telemedicine  \n",
            "Access \n",
            "Health policy a b s t r a c t \n",
            "Background:  Telemedicine  diagnoses  and treats patients  remotely  via telecommunications  technology  all \n",
            "over the world. Telemedicine  becomes  more prevalent  as providers  recognize  the beneﬁts,  patients  re- \n",
            "ceive increased  access and payers see the reduction  in cost of care. \n",
            "Objective:  Telemedicine  studies have shown success in limiting  geographical  constraints,  time spent, and \n",
            "costs incurred  by patients  with positive  health outcomes  across medical  specialties.  The aim of this re- \n",
            "view is to evaluate  the implications  of telemedicine  and health policies.  \n",
            "Methods:  An assessment  of the literature  in four databases  was made on content  germane  to health \n",
            "policy implications  of telemedicine.  From the results of the search, 48 publications  were kept for analysis.  \n",
            "Results: The ﬁfteen facilitators  mentioned  most often were increased  access, increased  convenience,  im- \n",
            "proved population  health, care enabled  through  mobile technology,  self-eﬃcacy,  increased  patient-to-  \n",
            "provider  communication,  cost advantages,  eﬃcacy of modality,  increased  health outcomes,  reaches de- \n",
            "veloping  countries,  increased  quality, a positive  previous  experience,  and a secure means of care. The \n",
            "twelve barriers  mentioned  most often were the increased  cost to providers,  patient privacy,  technical  lit- \n",
            "eracy, state licensing,  data security,  socioeconomics,  limited reimbursements,  issues of interoperability,  \n",
            "patient safety, less personal  means of care, misaligned  incentives,  and ethical concerns.  \n",
            "Conclusions:  Telemedicine  has the potential  for growth and adoption,  however,  there are several implica-  \n",
            "tions and barriers  of health policy surrounding  telemedicine  that make it diﬃcult  to adopt. Policies will \n",
            "likely encourage  and incentivize  its spread and use. Future research  should focus on standardization  of \n",
            "telemedicine  and new policies  and incentives  that encourage  its use. \n",
            "©2 0 2 0 Fellowship  of Postgraduate  Medicine.  Published  by Elsevier  Ltd. \n",
            "This is an open access article under the CC BY-NC-ND  license \n",
            "( http://creativecommons.org/licenses/by-nc-nd/4.0/  ) \n",
            "Introduction  \n",
            "Rationale  \n",
            "Health policy and telemedicine  are heavily  intertwined  just as \n",
            "any innovation  needs to be regulated  for eﬃcacy  and safety, pay- \n",
            "ment models  need to be established  and implemented,  and privacy  \n",
            "and security  need to be maintained  [1–3] . With telemedicine  and \n",
            "new legislative  health policies,  outcomes  can be improved  with \n",
            "greater  eﬃciency  for both patient  and provider  [2–4] . This can be \n",
            "Abbreviations:  ACA, affordable  care act; CINAHL, cumulative  index of nursing \n",
            "and allied health literature;  HIT, health information  technology;  HITECH, health \n",
            "information  technology  for economic  and clinical health; MeSH, medical subject \n",
            "headings;  PRISMA, preferred  reporting  items for systematic  reviews and meta- \n",
            "analyses;  WHO, World Health Organization;  WoS, web of science. \n",
            "∗Corresponding  author at: School of Health Administration,  Texas State Univer- \n",
            "sity, 601 University  Dr, Encino Hall, Room 250, San Marcos, TX 78666, United States. \n",
            "E-mail address: scottkruse@txstate.edu  (C.S. Kruse). \n",
            "1 These authors contributed  equally. done by reaching  patients  in a more comfortable  setting,  removing  \n",
            "travel and time constraints,  while improving  the patient’s  health \n",
            "and experience.  Telemedicine  has been creeping  into the health-  \n",
            "care ﬁeld for decades,  but diffusion  of this innovation  will likely \n",
            "improve  with the implementation  of additional  health policy such \n",
            "as the Affordable  Care Act (ACA) and the Health Information  Tech- \n",
            "nology for Economic  and Clinical  Health (HITECH)  Act in Amer- \n",
            "ica which expanded  organizational  models  that use telemedicine  \n",
            "and broadened  reimbursement  mechanisms  for the same [5 , 6] . It \n",
            "is logical that providers  would examine  all options  of eﬃcacious  \n",
            "care and gravitate  to those that are cost effective,  but health pol- \n",
            "icy must enable the innovation,  establish  basic standards  of safety \n",
            "and security,  and reduce boundaries  to their adoption.  \n",
            "Telemedicine  alters the modality  of delivery  for healthcare.  \n",
            "Care is still provided  in a high-quality  manner,  but it is delivered  \n",
            "through  telecommunications  technology  [7] . This modality  be- \n",
            "comes easier and less costly with the spread of technology,  speciﬁ-  \n",
            "cally smartphones  [8] . Telemedicine  is the solution  to the problem  \n",
            "of proximate  distance  between  providers  and patients  with mobil- \n",
            "https://doi.org/10.1016/j.hlpt.2020.10.006  \n",
            "2211-8837/© 2020 Fellowship of Postgraduate Medicine. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license  \n",
            "( http://creativecommons.org/licenses/by-nc-nd/4.0/  ) \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "folder = '/content/drive/MyDrive/D2K_Data'"
      ],
      "metadata": {
        "id": "NvRnQitIpwuy"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "for i in range(0,18):\n",
        "  reader = PdfReader(f'{folder}/sample{i}.pdf')\n",
        "  number_of_pages = len(reader.pages)\n",
        "  li = ''\n",
        "  for j in range(number_of_pages):\n",
        "    page = reader.pages[j]\n",
        "    page_content = page.extract_text()\n",
        "    li += page_content\n",
        "  data.append({f'sample{i}':li})"
      ],
      "metadata": {
        "id": "qgs3hNclwNPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "991WxM2rw-Sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Text Summarization**###"
      ],
      "metadata": {
        "id": "7KpwBt7u5_0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "id": "TKDb4vcAxuhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "a3_2U_5_3meh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from string import punctuation"
      ],
      "metadata": {
        "id": "Znq2KBe_3VTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "w9rH_BxD3bnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "DPY-Ot9W3fcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(data[0]['sample0'])"
      ],
      "metadata": {
        "id": "FU6o_P6h3sEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "0_QZxECQ3y0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "metadata": {
        "id": "UpezycU935ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1\n",
        "                \n",
        "print(word_frequencies)"
      ],
      "metadata": {
        "id": "bMXjp_kj4C6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency"
      ],
      "metadata": {
        "id": "BkACjv3r4Tf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word]/max_frequency\n",
        "\n",
        "print(word_frequencies)"
      ],
      "metadata": {
        "id": "wArb5UlR4XC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = [sent for sent in doc.sents]\n",
        "print(sentence_tokens)"
      ],
      "metadata": {
        "id": "u5SuWZkL4aF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_tokens:\n",
        "    for word in sent:\n",
        "        if word.text.lower() in word_frequencies.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = word_frequencies[word.text.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent] += word_frequencies[word.text.lower()]\n",
        "                \n",
        "sentence_scores"
      ],
      "metadata": {
        "id": "NRstjk-84mLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest"
      ],
      "metadata": {
        "id": "X-0Izxl6411W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "select_length = int(len(sentence_tokens)*0.3)\n",
        "select_length"
      ],
      "metadata": {
        "id": "EJaNKvtU5Ao3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Health care Domain keywords tokenzation"
      ],
      "metadata": {
        "id": "NMVfoX_BjlsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = ['Artificial intelligence in healthcare', 'Big data analytics in healthcare',  'Clinical decision support systems', 'Electronic health records', 'Health information technology',             'Health insurance', 'Health policy', 'Health promotion', 'Health services research', 'Healthcare financing',             'Healthcare management', 'Healthcare quality improvement', 'Healthcare regulation', 'Healthcare technology',             'Hospital administration', 'Medical ethics', 'Medical informatics', 'Medical imaging', 'Medical research',             'Mental health', 'Nutrition and dietetics', 'Occupational therapy', 'Patient safety', 'Pediatrics', 'Pharmacy',             'Physical therapy', 'Public health', 'Quality of life', 'Radiology', 'Rehabilitation', 'Reproductive health',             'Social work', 'Telemedicine', 'Vaccination', \"Women's health\", 'Aging and geriatrics', 'Alternative medicine',             'Cancer research', 'Cardiovascular diseases', 'Chronic diseases', 'Complementary medicine', 'Dental health',             'Dermatology', 'Endocrinology', 'Epidemiology', 'Gastroenterology', 'Genetics', 'Infectious diseases', 'Neurology',             'Respiratory diseases']\n"
      ],
      "metadata": {
        "id": "YCsTJ7hVjlLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores"
      ],
      "metadata": {
        "id": "6-fVCCre9Ds7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "summary"
      ],
      "metadata": {
        "id": "6QHRCt7q5EjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t=set(summary)\n",
        "t"
      ],
      "metadata": {
        "id": "NEK1FHPU5HKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLTK**"
      ],
      "metadata": {
        "id": "G5m3D69RHyxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "0iVpVQtGH1Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n"
      ],
      "metadata": {
        "id": "5evQm-ndEOte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "id": "A5fRGaomH6a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = punctuation + '\\n'\n",
        "punctuation"
      ],
      "metadata": {
        "id": "oPAeiTJKIkLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_frequencies = {}\n",
        "for word in doc:\n",
        "    if word.text.lower() not in stopwords:\n",
        "        if word.text.lower() not in punctuation:\n",
        "            if word.text not in word_frequencies.keys():\n",
        "                word_frequencies[word.text] = 1\n",
        "            else:\n",
        "                word_frequencies[word.text] += 1\n",
        "                \n",
        "print(word_frequencies)"
      ],
      "metadata": {
        "id": "dPGcDts3ImLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_frequency = max(word_frequencies.values())\n",
        "max_frequency"
      ],
      "metadata": {
        "id": "q8Q68rOSgMAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = word_frequencies[word]/max_frequency"
      ],
      "metadata": {
        "id": "6Ez8Zb7TIyRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_frequencies)"
      ],
      "metadata": {
        "id": "amNOY5SjI0ZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_token = sent_tokenize(data[0]['sample0'])\n",
        "sent_token"
      ],
      "metadata": {
        "id": "krlO6VzUI3Lf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores = {}\n",
        "for sent in sent_token:\n",
        "    sentence = sent.split(\" \")\n",
        "    for word in sentence:        \n",
        "        if word.lower() in word_frequencies.keys():\n",
        "            if sent not in sentence_scores.keys():\n",
        "                sentence_scores[sent] = word_frequencies[word.lower()]\n",
        "            else:\n",
        "                sentence_scores[sent] += word_frequencies[word.lower()]"
      ],
      "metadata": {
        "id": "ZX4cCBa9I-WO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent_token)"
      ],
      "metadata": {
        "id": "-88UKS-rKjcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_scores"
      ],
      "metadata": {
        "id": "LYxOqLS7JIAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from heapq import nlargest\n",
        "select_length = int(len(sent_token)*0.3)\n",
        "select_length"
      ],
      "metadata": {
        "id": "vXXNNbeoJKJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = nlargest(select_length, sentence_scores, key = sentence_scores.get)\n",
        "final_summary = [word for word in summary]\n",
        "summary = ' '.join(final_summary)\n",
        "summary = summary.replace('\\n',' ').replace('\\r',' ')\n",
        "summary"
      ],
      "metadata": {
        "id": "7nuHKVUhJRZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n"
      ],
      "metadata": {
        "id": "56mfTrcCfZwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "\n",
        "def evaluate_summarizer(reference, summary):\n",
        "    \"\"\"\n",
        "    Evaluates a text summarizer using the Rouge algorithm.\n",
        "\n",
        "    Args:\n",
        "    - reference (str): the reference text to summarize\n",
        "    - summary (str): the summary generated by the text summarizer\n",
        "\n",
        "    Returns:\n",
        "    - rouge_scores (dict): a dictionary of Rouge scores (Rouge-1, Rouge-2, Rouge-L) for the summary\n",
        "    \"\"\"\n",
        "    scores = rouge.get_scores(summary, reference)\n",
        "    rouge_scores = {\n",
        "        \"rouge-1\": scores[0][\"rouge-1\"],\n",
        "        \"rouge-2\": scores[0][\"rouge-2\"],\n",
        "        \"rouge-l\": scores[0][\"rouge-l\"],\n",
        "    }\n",
        "    return rouge_scores"
      ],
      "metadata": {
        "id": "Yal97JYUfLRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_summarizer(data[0]['sample0'],summary)"
      ],
      "metadata": {
        "id": "BTLmotkqfkNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def summarize_section(section_text):\n",
        "    \"\"\"\n",
        "    Given the text of a section, generate a summary using NLP and machine learning techniques.\n",
        "    \"\"\"\n",
        "    # Remove non-alphanumeric characters and convert to lowercase\n",
        "    section_text = ''.join(c for c in section_text if c.isalnum() or c.isspace())\n",
        "    section_text = section_text.lower()\n",
        "    \n",
        "    # Tokenize and count the frequency of each word\n",
        "    doc = nlp(section_text)\n",
        "    word_counts = Counter(token.text for token in doc if not token.is_stop and not token.is_punct)\n",
        "    \n",
        "    # Extract the top 5 most frequent words and return them as the summary\n",
        "    summary = [word for word, count in word_counts.most_common(5)]\n",
        "    summary = \" \".join(summary)\n",
        "    \n",
        "    return summary\n",
        "\n",
        "\n",
        "def summarize_paper(paper_text):\n",
        "    \"\"\"\n",
        "    Given the text of a research paper, generate section-wise summaries using NLP and machine learning techniques.\n",
        "    \"\"\"\n",
        "    # Split the paper into sections\n",
        "    sections = paper_text.split(\"\\n\\n\")\n",
        "    section_names = [\"Abstract\", \"Methodology\", \"Evaluation\", \"Conclusion\", \"Discussion\"]\n",
        "    \n",
        "    # Generate a summary for each section\n",
        "    summaries = {}\n",
        "    for i in range(len(sections)):\n",
        "        section_name = section_names[i] if i < len(section_names) else \"Additional section\"\n",
        "        section_text = sections[i]\n",
        "        section_summary = summarize_section(section_text)\n",
        "        summaries[section_name] = section_summary\n",
        "    \n",
        "    return summaries\n",
        "\n",
        "# Example usage\n",
        "paper_text = \"\"\"\n",
        "Abstract: This paper presents a new method for sentiment analysis using deep learning. We propose a novel architecture that combines convolutional and recurrent neural networks to capture both local and global context. Our experiments on a benchmark dataset show that our method outperforms state-of-the-art approaches in terms of accuracy and efficiency.\n",
        "\n",
        "Methodology: We collected a corpus of 10,000 movie reviews and preprocessed the text using standard techniques such as tokenization, stopword removal, and stemming. We then split the corpus into training and testing sets and trained our sentiment analysis model using TensorFlow. The model consists of 3 convolutional layers and 2 LSTM layers, with a final softmax layer for classification.\n",
        "\n",
        "Evaluation: We evaluated our model on the popular IMDB sentiment analysis dataset and achieved an accuracy of 90%, outperforming the previous state-of-the-art method which had an accuracy of 87%. We also conducted ablation studies to investigate the contribution of each component in our model and found that both convolutional and recurrent layers are important for achieving high accuracy.\n",
        "\n",
        "Conclusion: In conclusion, we presented a new method for sentiment analysis using a combination of convolutional and recurrent neural networks. Our experiments on a benchmark dataset showed that our method outperforms state-of-the-art approaches in terms of accuracy and efficiency.\n",
        "\n",
        "Discussion: We believe that our method has the potential to be applied to other natural language processing tasks and lead to further improvements in the field. However, there are still some limitations to our approach, such as the need for large amounts of labeled data and the relatively high computational requirements of deep learning models.\n",
        "\"\"\"\n",
        "summaries = summarize_paper(paper_text)\n",
        "\n",
        "print(\"Abstract summary:\", summaries[\"Abstract\"])\n",
        "print(\"Methodology summary:\", summaries[\"Methodology\"])\n",
        "print(\"Evaluation summary:\", summaries[\"Evaluation\"])\n",
        "print(\"Conclusion summary:\", summaries[\"Conclusion\"])\n",
        "print(\"Discussion summary:\", summaries[\"Discussion\"])"
      ],
      "metadata": {
        "id": "71BiF4Z_f6bo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cAIMlfhOkjmM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}